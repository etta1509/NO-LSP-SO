function [xk , fk , gradfk_norm , k, xseq , btseq , rate_convergence ] = truncated_newton_pre(x0, f, gradf, Hessf, kmax, tolgrad, c1, rho, btmax, varargin)
% TRUNCATED_NEWTON_PRE - Truncated Newton method with backtracking and preconditioning .
% Optimized for large - scale problems and sparse Hessian matrices .
%
% [xk , fk , gradfk_norm , k, xseq , btseq , convergence_rate ] = truncated_newton_pre (x0 , f, gradf , Hessf , kmax , tolgrad , c1 , rho , btmax , varargin )
%
% This function implements a Preconditioned Truncated Newton method for unconstrained optimization
% of a function f: R^n -> R. It is specifically designed for large - scale problems where the Hessian
% matrix is sparse , and employs preconditioning to accelerate convergence . A backtracking line search
% based on the Armijo condition is used to ensure sufficient decrease in the function value at each iteration .
% The Hessian - vector product within the Conjugate Gradient (CG) solver is used , and a preconditioner
% can be applied within the CG iterations ( though the provided code for cg_curvtrun_newt_pre is not given ,
% it 's assumed that preconditioning is applied there ).
%
% INPUTS :
% x0 - Starting point for the optimization ( column vector )
% f - Function handle for the objective function f: R^n -> R
% gradf - Function handle for the gradient of f
% Hessf - Function handle that , given x, returns the sparse Hessian matrix , or
% a constant sparse matrix if the Hessian is independent of x.
% kmax - Maximum number of iterations for the outer Newton loop
% tolgrad - Tolerance for the gradient norm ( stopping criterion )
% c1 - Parameter for the Armijo condition ( typically 1e -4)
% rho - Step reduction factor for backtracking (0 < rho < 1, e.g., 0.5)
% btmax - Maximum number of step reductions in backtracking line search
% varargin - ( Optional ) Variable input arguments ; may be used to pass preconditioning matrix
% or parameters to the cg_curvtrun_newt_pre function ( not directly used in this code snippet ,
% but included for potential future extensibility ).
%
% OUTPUTS :
% xk - The last computed iterate ( approximation of the minimizer )
% fk - Function value f(xk) at the final iterate
% gradfk_norm - Norm of the gradient at xk , || gradf (xk)||
% k - Total number of iterations performed by the Newton method
% xseq - Sequence of iterates generated by the method ( each column is an iterate )
% btseq - Sequence of backtracking iterations performed at each Newton iteration
% rate_convergence - Estimate of the rate of convergence ( average of estimated orders in
% the last iterations )


%% Input Preparation
% Ensure the starting point x0 is a column vector
x0 = x0(:);
% Get the dimension of the problem from the starting point
n = numel(x0);


%% Pre - allocation for iteration history
% Initialize an array to store the sequence of iterates
xseq = zeros(n, kmax);
% Initialize an array to store the number of backtracking steps at each iteration
btseq = zeros(1, kmax);


%% Initialization
% Set the initial iterate to the starting point
xk = x0;
% Evaluate the objective function at the starting point
fk = f(xk);
% Evaluate the gradient at the starting point
gradfk = gradf(xk);
% Compute the norm of the gradient at the starting point
gradfk_norm = norm(gradfk);


% Check if Hessf is a function handle or a matrix
if isa (Hessf , ' function_handle ')
    hessfk = Hessf(xk); % If function handle , evaluate Hessian at xk ( expecting a sparse
    % matrix )
else
    hessfk = Hessf; % If not a function handle , assume Hessf is already the sparse
    % Hessian matrix ( constant )
end


% Initialize iteration counter
k = 0;
% Define a function handle for the Armijo condition right - hand side for efficiency
armijo_rhs = @(fk , alpha , gradfk , pk) fk + c1 * alpha * ( gradfk' * pk);


% Main loop of the Preconditioned Truncated Newton method
while (k < kmax ) && ( gradfk_norm >= tolgrad )
    % Dynamic tolerance for Conjugate Gradient based on current gradient norm
    etak = min(0.5 , gradfk_norm );

    % Solve the system Hessf (xk)*p = -gradf (xk) using Preconditioned Truncated CG
    % ( cg_curvtrun_newt_pre function , assumed to be defined elsewhere and implementing
    % preconditioning )
    pk = cg_curvtrun_newt_pre( hessfk , -gradfk , zeros(n ,1) , etak );

    % -----------------------
    % Backtracking Line Search ( Armijo condition )
    % -----------------------
    % Initialize step size to 1
    alpha = 1;
    % Calculate the new point xnew with full step size
    xnew = xk + alpha * pk;
    % Evaluate the function at the new point
    fnew = f( xnew );
    % Initialize backtracking iteration counter
    bt = 0;
    % Backtracking loop : reduce step size until Armijo condition is satisfied or max
    % backtrack iterations reached
    while (bt < btmax ) && ( fnew > armijo_rhs (fk , alpha , gradfk , pk))
        % Reduce step size by factor rho
        alpha = rho * alpha ;
        % Recalculate the new point xnew with reduced step size
        xnew = xk + alpha * pk;
        % Re - evaluate the function at the new point
        fnew = f( xnew );
        % Increment backtracking iteration counter
        bt = bt + 1;
    end

    % Warning if maximum backtracking iterations are reached without satisfying Armijo condition
    if (bt == btmax ) && ( fnew > armijo_rhs (fk , alpha , gradfk , pk))
        warning ('Maximum backtracking iterations (%d) reached at iteration %d: Armijo condition not satisfied .', btmax , k +1) ;
    end

    % Update variables for the next iteration
    % New iterate xk is set to xnew
    xk = xnew ;
    % Function value at new iterate
    fk = fnew ;
    % Gradient at new iterate
    gradfk = gradf (xk);
    % Gradient norm at new iterate
    gradfk_norm = norm ( gradfk );

    % Update Hessian matrix (if Hessf is a function handle , re - evaluate at new xk)
    if isa (Hessf , ' function_handle ')
        hessfk = Hessf (xk); % Re - evaluate Hessian at xk if it 's a function handle
    else
        hessfk = Hessf ; % Keep Hessian as constant matrix if it 's not a function handle
    end


    % Increment iteration counter
    k = k + 1;
    % Store current iterate in xseq
    xseq (:, k) = xk;
    % Store number of backtracking steps in btseq
    btseq (k) = bt;
end

%% Output Processing
% Resize output arrays to remove pre - allocated space
xseq = xseq (:, 1:k);
btseq = btseq (1: k);


% Estimate rate of convergence ( vectorized calculation for efficiency )
x_star = xseq (:, end ); % Assume last iterate is close to x*
e = vecnorm ( xseq - x_star , 2, 1); % Error at each iteration
% Find indices where errors are sufficiently larger than machine precision
valid = (e(1: end -2) > 1e-12) & (e(2: end -1) > 1e-12) & (e(3: end ) > 1e-12) ;
if any(valid)
    % Estimate convergence order using errors from consecutive iterations
    p_est = log(e(3: end )./e(2: end -1) ) ./ log(e(2: end -1) ./e(1: end -2));
    % Average the estimated orders for a single rate of convergence value
    rate_convergence = mean( p_est( valid ));
else
    rate_convergence = NaN; % Return NaN if rate cannot be reliably estimated
end
end
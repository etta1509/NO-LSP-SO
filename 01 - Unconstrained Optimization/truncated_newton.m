function [xk , fk , gradfk_norm , k, xseq , btseq , rate_convergence ] = truncated_newton(x0, f, gradf, Hessf, kmax, tolgrad, c1, rho, btmax)
% TRUNCATED_NEWTON Truncated Newton method with backtracking ( Armijo ) without preconditioning
%
% [xk , fk , gradfk_norm , k, xseq , btseq , convergence_rate ] = truncated_newton (x0 , f, gradf ,
% Hessf , kmax , tolgrad , c1 , rho , btmax )
%
% This function implements the Truncated Newton method for unconstrained optimization
% of a function f: R^n -> R. It uses a backtracking line search based on the Armijo
% condition to ensure sufficient decrease in the function value at each iteration .
% The Hessian - vector product is approximated using Conjugate Gradient , making it suitable
% for large - scale problems where forming and storing the full Hessian is impractical .
% Preconditioning is NOT used in this version .
%
% INPUTS :
% x0 - Starting point for the optimization ( column vector )
% f - Function handle for the objective function f: R^n -> R
% gradf - Function handle for the gradient of f
% Hessf - Function handle for the Hessian of f ( used for Hessian - vector product in
% CG)
% kmax - Maximum number of iterations for the outer Newton loop
% tolgrad - Tolerance for the gradient norm ( stopping criterion )
% c1 - Parameter for the Armijo condition (in (0 ,1) , e.g., 1e -4)
% rho - Reduction factor for backtracking line search (in (0 ,1) , e.g., 0.5)
% btmax - Maximum number of iterations for the backtracking line search
%
% OUTPUTS :
% xk - The last computed iterate ( approximation of the minimizer )
% fk - Function value f(xk) at the final iterate
% gradfk_norm - Norm of the gradient at xk , || gradf (xk)||
% k - Total number of iterations performed by the Newton method
% xseq - Sequence of iterates generated by the method ( each column is an
% iterate )
% btseq - Sequence of backtracking iterations performed at each Newton iteration
% rate_convergence - Estimate of the rate of convergence ( average of estimated orders in
% the last iterations )


%% Input Validation
% Ensure the starting point x0 is a column vector
x0 = x0(:);
% Get the dimension of the problem from the starting point
n = numel(x0);

%% Pre - allocation for iteration history
% Initialize an array to store the sequence of iterates
xseq = zeros(n, kmax);
% Initialize an array to store the number of backtracking steps at each iteration
btseq = zeros(1, kmax);

%% Initialization
% Set the initial iterate to the starting point
xk = x0;
% Evaluate the objective function at the starting point
fk = f(xk);
% Evaluate the gradient at the starting point
gradfk = gradf(xk);
% Compute the norm of the gradient at the starting point
gradfk_norm = norm(gradfk);
% Evaluate the Hessian at the starting point ( for use in CG)
hessfk = Hessf(xk);
% Initialize iteration counter
k = 0;

% Main loop of the Truncated Newton method
while (k < kmax ) && ( gradfk_norm >= tolgrad )
    % Dynamic tolerance for Conjugate Gradient based on current gradient norm
    etak = min (0.5 , gradfk_norm );

    % Compute the search direction pk by approximately solving Hessf (xk)*pk = -gradf (xk)
    % using Conjugate Gradient method ( cg_curvtrun_newt function , assumed to be defined
    % elsewhere )
    pk = cg_curvtrun_newt ( hessfk , -gradfk , zeros (n ,1) , etak );

    % Check if the computed direction is a descent direction
    % If not (i.e., not a descent direction ), fall back to steepest descent direction (-
    % gradf (xk))
    if gradfk' * pk >= 0
        warning('Computed direction is not a descent direction . Using steepest descent direction : -gradf .');
        pk = -gradf (xk);
    end

    % Backtracking line search ( Armijo condition )
    % Initialize step size to 1
    alpha = 1;
    % Compute the Armijo condition target value
    curr_armijo = fk + c1 * alpha * ( gradfk' * pk);
    % Calculate the new point xnew and function value fnew with current step size
    xnew = xk + alpha * pk;
    fnew = f( xnew );
    % Initialize backtracking iteration counter
    bt = 0;
    % Backtracking loop : reduce step size until Armijo condition is satisfied or max
    % backtrack iterations reached
    while (bt < btmax ) && ( fnew > curr_armijo )
        % Reduce step size by factor rho
        alpha = rho * alpha ;
        % Update Armijo condition target value with reduced step size
        curr_armijo = fk + c1 * alpha * ( gradfk' * pk);
        % Recalculate xnew and fnew with updated step size
        xnew = xk + alpha * pk;
        fnew = f( xnew );
        % Increment backtracking iteration counter
        bt = bt + 1;
    end

% Warning if maximum backtracking iterations are reached without satisfying Armijo condition
if bt == btmax && fnew > curr_armijo
    warning ('Maximum backtracking iterations (%d) reached at iteration %d: Armijo condition not satisfied .', btmax , k +1);
end

% Update variables for the next iteration
% New iterate xk is set to xnew
xk = xnew ;
% Function value at new iterate
fk = fnew ;
% Gradient at new iterate
gradfk = gradf(xk);
% Gradient norm at new iterate
gradfk_norm = norm(gradfk);
% Hessian at new iterate ( for use in next CG iteration )
hessfk = Hessf(xk);

% Increment iteration counter
k = k + 1;
% Store current iterate in xseq
xseq (:, k) = xk;
% Store number of backtracking steps in btseq
btseq (k) = bt;
end

% Resize output arrays to remove pre - allocated space
xseq = xseq (:, 1:k);
btseq = btseq (1: k);

% Estimate rate of convergence ( vectorized calculation for efficiency )
x_star = xseq (:, end ); % Assume last iterate is close to x*
e = vecnorm ( xseq - x_star , 2, 1); % Error at each iteration
% Find indices where errors are sufficiently larger than machine precision to avoid log (0)
% or division by zero
valid = (e (1: end -2) > 1e-12) & (e(2: end -1) > 1e-12) & (e(3: end ) > 1e-12) ;
if any ( valid )
    % Estimate convergence order using errors from consecutive iterations
    p_est = log (e (3: end )./e (2: end -1) ) ./ log(e(2: end -1) ./e (1: end -2) );
    % Average the estimated orders for a single rate of convergence value
    rate_convergence = mean ( p_est ( valid ));
else
    rate_convergence = NaN; % Not a Number if rate cannot be reliably estimated
end
end